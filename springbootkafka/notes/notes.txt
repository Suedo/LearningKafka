Course:
https://www.udemy.com/course/apache-kafka-for-developers-using-springboot/

git:
https://github.com/dilipsundarraj1/kafka-for-developers-using-spring-boot

------------------------------------------------------------------------------------------------------------------------

docker run -d  --net=host  --name=kafka  -e KAFKA_ZOOKEEPER_CONNECT=localhost:32181  -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://localhost:29092  -e KAFKA_BROKER_ID=2  -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1  confluentinc/cp-kafka:5.5.1

------------------------------------------------------------------------------------------------------------------------
Zookeeper:
------------------------------------------------------------------------------------------------------------------------
https://www.youtube.com/watch?v=SxHsnNYxcww
https://zookeeper.apache.org/doc/r3.6.1/zookeeperStarted.html

Some important properties (with some standard defaults) for configuration:
------------------------------------------
    1. ticktime: 2000
        the basic time unit in milliseconds used by ZooKeeper. It is used to do heartbeats and the minimum session timeout will be twice the tickTime.

    2. initlimit: 5
        limit the length of time, in `ticks` the ZooKeeper servers in quorum have to connect to a leader
        in other words, time in ticks a zookeeper follower takes to connect to a zookeeper leader initially when a cluster is started

    3. syncLimit: 2
        limits how far (in ticks) out of date a server can be from a leader.
        In other words, it's the time limit a follower has to sync with the leader

    4. server.X=localhost:port1:port2
    EX:
        server.1=localhost:2888:3888
        server.2=localhost:2888:3888
        server.3=localhost:2888:3888

        port1 is used for zookeeper node peer communication, to agree upon the order of updates etc.
        port2 is used for leader election
        If these zookeeper nodes are deployed on different boxes, the ports can be the same, with localhost replaced with ip

    5. clientPort: 2181
        the port to listen for client connections, ie This is the port to which clients (like kafka) will connect to

------------------------------------------------------------------------------------------------------------------------
    Zookeeper Ensemble
------------------------------------------------------------------------------------------------------------------------
https://www.youtube.com/watch?v=SxHsnNYxcww

Zookeeper maintains the entire Kafka cluster, so to prevent single point of failure, zookeeper also needs to be setup
with multiple nodes. The collection of zookeeper nodes is known as a zookeeper ensemble.

Concept of Quorum in zookeeper ensemble:
1. Advisable to have odd number of zookeeper node, it is more fault tolerant (?)
2. Nodes needed for quorum: CEIL(n/2), where n == total number of nodes in zookeeper

------------------------------------------------------------------------------------------------------------------------
Listeners:
------------------------------------------------------------------------------------------------------------------------

https://www.confluent.io/blog/kafka-listeners-explained/

> listeners : is what the broker will use to create server sockets.
> advertised.listeners : is what clients will use to connect to the brokers.
    https://stackoverflow.com/a/43000344/2715083

------------------------------------------------------------------------------------------------------------------------
Bootstrap Servers:
------------------------------------------------------------------------------------------------------------------------
    From a kafka client (producer/consumer) pov, bootstrap servers are the brokers it will connect to

------------------------------------------------------------------------------------------------------------------------
installer commands VS docker commands:
------------------------------------------------------------------------------------------------------------------------
docker link: https://kafka-tutorials.confluent.io/kafka-console-consumer-producer-basics/kafka.html
commands link: https://github.com/dilipsundarraj1/kafka-for-developers-using-spring-boot/blob/master/SetUpKafka.md

    How to create a topic ?
    ------------------------------------------
        ./kafka-topics.sh                       --create --topic test-topic -zookeeper localhost:2181 --replication-factor 1 --partitions 4

        docker-compose exec broker kafka-topics --create --topic test-topic --bootstrap-server broker:9092 --replication-factor 1 --partitions 1

    How to instantiate a Console Producer?
    ------------------------------------------
        ./kafka-console-producer.sh             --topic test-topic --broker-list localhost:9092

        docker-compose exec broker bash
        kafka-console-producer                  --topic test-topic --broker-list broker:9092

    How to instantiate a Console Consumer?
    ------------------------------------------
        ./kafka-console-consumer.sh     --topic test-topic --bootstrap-server localhost:9092 --from-beginning

        docker-compose exec broker bash
        kafka-console-consumer          --topic test-topic --bootstrap-server broker:9092


    kafka-topics --zookeeper localhost:2181 --describe
    kafka-topics --zookeeper localhost:2181 --list

------------------------------------------------------------------------------------------------------------------------
Udemy Course instuctions translated to docker
------------------------------------------------------------------------------------------------------------------------

./kafka-topics.sh --create --topic test-topic -zookeeper localhost:2181 --replication-factor 1 --partitions 4

./kafka-console-producer.sh --broker-list localhost:9092 --topic test-topic
./kafka-console-producer.sh --broker-list localhost:9092 --topic test-topic --property "key.separator=-" --property "parse.key=true"

./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test-topic --from-beginning
./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test-topic --from-beginning -property "key.separator= - " --property "print.key=true"

./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test-topic --group <group-name>

------------------------------------------------------------------------------------------------------------------------
Kafka Listeners – Explained : https://www.confluent.io/blog/kafka-listeners-explained/
------------------------------------------------------------------------------------------------------------------------

Apache Kafka is a distributed system. Data is read from and written to the leader for a given partition, which could be
on any of the brokers in a cluster. When a client (producer/consumer) starts, it will request metadata about which
broker is the leader for a partition—and it can do this from any broker. The metadata returned will include the
endpoints available for the Leader broker for that partition, and the client will then use those endpoints to connect to
the broker to read/write data as required


------------------------------------------------------------------------------------------------------------------------
Behind the scenes: KafkaTemplate.send()
------------------------------------------------------------------------------------------------------------------------

1. send()
2.     key.serializer, value.serializer
3.         DefaultPartitioner
4.             RecordAccumulator
                > #RecordBatch == #TopicPartition (helps parallelization)
                > Each RecordBatch has a batch.size
                > RecordAccumulator has an overall buffer.memory (bytes)

Conditions for which messages are sent to Kafka Topic:
> RecordBatch is full
> if RecordBatch is taking time to fill, linger.ms time will be waited, and then sent whatever amount of messages has accumulated
    https://docs.confluent.io/current/installation/configuration/producer-configs.html#linger.ms


------------------------------------------------------------------------------------------------------------------------
Creating a Topic programatically (not recommended for production
------------------------------------------------------------------------------------------------------------------------
Requires:
    1. @Bean of KafkaAdmin
    2. @Bean of NewTopic

------------------------------------------------------------------------------------------------------------------------
Misc
------------------------------------------------------------------------------------------------------------------------

How Kafka n Zookeeper communicate:
------------------------------------------------
https://stackoverflow.com/a/54015863/2715083
^^ Answer by Gwen Shapira, Kafka Product Manager @ Confluent

Apache Kafka vs Cofluent Kafka
------------------------------------------------
https://stackoverflow.com/a/39709900/2715083
^^ Answer by Gwen Shapira, Kafka Product Manager @ Confluent


Testing:
    When using lombok, rebuild project before testing as otherwise some generated getters and setters might not reflect in the test


Kafka insync replicas, acks and meaning:
https://www.cloudkarafka.com/blog/2019-09-28-what-does-in-sync-in-apache-kafka-really-mean.html

    acks=all
    Setting the ack value to all means that the producer gets an ack when all in-sync replicas have received the record.
    The leader will wait for the full set of in-sync replicas to acknowledge the record. This means that it takes a longer time to
    send a message with ack value all, but it gives the strongest message durability.

------------------------------------------------------------------------------------------------------------------------
Kafka Producer Config
------------------------------------------------------------------------------------------------------------------------

Acks:
    0 - none
    1 - leader only
    all - leader + all replicas, same as -1

Retries: 0 - to - 2147483647
    2147483647 == MAX value, is the default value in spring

retry.backoff.ms : 100 ms by default




------------------------------------------------------------------------------------------------------------------------
Consumer up, log obtained after producing on record
------------------------------------------------------------------------------------------------------------------------

 c.k.b.producer.LibraryEventProducer      : Message sent successfully for key null with value {"libraryEventId":null,"book":{"bookId":456,"bookName":"Kafka Using Spring Boot","bookAuthor":"Dilip"}}, in partition: 0
 c.k.b.consumer.LibraryEventsConsumer     : Consumer Record: ConsumerRecord(topic = library-events, partition = 0, leaderEpoch = 0, offset = 0, CreateTime = 1602299751365, serialized key size = -1, serialized value size = 103, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"libraryEventId":null,"book":{"bookId":456,"bookName":"Kafka Using Spring Boot","bookAuthor":"Dilip"}})


------------------------------------------------------------------------------------------------------------------------
Saving Kafka consumer read messages into a DB
------------------------------------------------------------------------------------------------------------------------

The onMessage handler receives a ConsumerRecord. ConsumerRecord.value() holds the serialized payload/message.
    Use Jackson ObjectMapper to read the message into a Domain Model
        persist the domain model using JPA save method

Imp: sample library event post body:

    {
        "book": {
            "bookName": "Kafka Using Spring Boot",
            "bookAuthor": "Dilip"
        }
    }

Notice that neither has any ID, (earlier, before introducing persistence, it did have id). Keeping id will lead to failures, or extra rows additions

------------------------------------------------------------------------------------------------------------------------
Recovery from exhausted retry attempts
------------------------------------------------------------------------------------------------------------------------

At the consumer side, when we have exhausted the number of times we configured for retry attempts,
we will check if the consumed record is recoverable, if yes, we will try for recovery

Two types of recovery is done generally:
1. Produce a message to a dead letter topic, or the original topic, in which case, the message will go through the cycle from start again
2. Produce a message to a recovery topic, which can be consumed by recovery specific consumers

------------------------------------------------------------------------------------------------------------------------
Recovery method 1:
sending error record to the original topic. This causes an infinite loop, as seen below,
where we have 3 retry attempts, triggered by a `RecoverableDataAccessException`, and after 3 reties, it is followed by a recovery attempt
the recovery attempt again causes the same exception, leading to retry attempts, as nothing is done to catch it.. and will go on infinitely
------------------------------------------------------------------------------------------------------------------------

2020-10-17 07:55:49.183  INFO 51297 --- [ad | producer-1] c.k.b.service.LibraryEventService        : Recovery Message sent successfully for key 11 with value {"libraryEventId":11,"type":"UPDATE","book":{"bookId":1,"bookName":"Kafka Using Spring Boot 2","bookAuthor":"Dilip S","libraryEvent":null}}

2020-10-17 07:55:49.183  INFO 51297 --- [ntainer#0-0-C-1] .k.b.c.LibraryEventsConsumerManualOffset : Consumer Record: ConsumerRecord(topic = library-events, partition = 0, leaderEpoch = 0, offset = 1, CreateTime = 1602901549173, serialized key size = 4, serialized value size = 139, headers = RecordHeaders(headers = [], isReadOnly = false), key = 11, value = {"libraryEventId":11,"type":"UPDATE","book":{"bookId":1,"bookName":"Kafka Using Spring Boot 2","bookAuthor":"Dilip S","libraryEvent":null}})
2020-10-17 07:55:49.183  INFO 51297 --- [ntainer#0-0-C-1] c.k.b.service.LibraryEventService        : LibraryEvent consumed: LibraryEvent(libraryEventId=11, type=UPDATE)
Hibernate: select libraryeve0_.library_event_id as library_1_1_0_, libraryeve0_.book_id as book_id3_1_0_, libraryeve0_.type as type2_1_0_, book1_.book_id as book_id1_0_1_, book1_.book_author as book_aut2_0_1_, book1_.book_name as book_nam3_0_1_ from library_event libraryeve0_ left outer join book book1_ on libraryeve0_.book_id=book1_.book_id where libraryeve0_.library_event_id=?

2020-10-17 07:55:51.188  INFO 51297 --- [ntainer#0-0-C-1] .k.b.c.LibraryEventsConsumerManualOffset : Consumer Record: ConsumerRecord(topic = library-events, partition = 0, leaderEpoch = 0, offset = 1, CreateTime = 1602901549173, serialized key size = 4, serialized value size = 139, headers = RecordHeaders(headers = [], isReadOnly = false), key = 11, value = {"libraryEventId":11,"type":"UPDATE","book":{"bookId":1,"bookName":"Kafka Using Spring Boot 2","bookAuthor":"Dilip S","libraryEvent":null}})
2020-10-17 07:55:51.189  INFO 51297 --- [ntainer#0-0-C-1] c.k.b.service.LibraryEventService        : LibraryEvent consumed: LibraryEvent(libraryEventId=11, type=UPDATE)
Hibernate: select libraryeve0_.library_event_id as library_1_1_0_, libraryeve0_.book_id as book_id3_1_0_, libraryeve0_.type as type2_1_0_, book1_.book_id as book_id1_0_1_, book1_.book_author as book_aut2_0_1_, book1_.book_name as book_nam3_0_1_ from library_event libraryeve0_ left outer join book book1_ on libraryeve0_.book_id=book1_.book_id where libraryeve0_.library_event_id=?

2020-10-17 07:55:53.191  INFO 51297 --- [ntainer#0-0-C-1] .k.b.c.LibraryEventsConsumerManualOffset : Consumer Record: ConsumerRecord(topic = library-events, partition = 0, leaderEpoch = 0, offset = 1, CreateTime = 1602901549173, serialized key size = 4, serialized value size = 139, headers = RecordHeaders(headers = [], isReadOnly = false), key = 11, value = {"libraryEventId":11,"type":"UPDATE","book":{"bookId":1,"bookName":"Kafka Using Spring Boot 2","bookAuthor":"Dilip S","libraryEvent":null}})
2020-10-17 07:55:53.192  INFO 51297 --- [ntainer#0-0-C-1] c.k.b.service.LibraryEventService        : LibraryEvent consumed: LibraryEvent(libraryEventId=11, type=UPDATE)
Hibernate: select libraryeve0_.library_event_id as library_1_1_0_, libraryeve0_.book_id as book_id3_1_0_, libraryeve0_.type as type2_1_0_, book1_.book_id as book_id1_0_1_, book1_.book_author as book_aut2_0_1_, book1_.book_name as book_nam3_0_1_ from library_event libraryeve0_ left outer join book book1_ on libraryeve0_.book_id=book1_.book_id where libraryeve0_.library_event_id=?

2020-10-17 07:55:53.194  INFO 51297 --- [ntainer#0-0-C-1] c.k.b.c.LibraryEventsConsumerConfig      : Inside recoverable logic
2020-10-17 07:55:53.204  INFO 51297 --- [ad | producer-1] c.k.b.service.LibraryEventService        : Recovery Message sent successfully for key 11 with value {"libraryEventId":11,"type":"UPDATE","book":{"bookId":1,"bookName":"Kafka Using Spring Boot 2","bookAuthor":"Dilip S","libraryEvent":null}}

------------------------------------------------------------------------------------------------------------------------
Error Handling and Retry in Producer
In Sync Replica (ISR) using the "min.insync.replica" flag
------------------------------------------------------------------------------------------------------------------------

More about ISR: https://www.cloudkarafka.com/blog/2019-09-28-what-does-in-sync-in-apache-kafka-really-mean.html

We alter our library-events topic to have min.insync.replicas=2

Course uses the alter command on an already up topic:
    ./kafka-configs.sh --alter --zookeeper localhost:2181 --entity-type topics --entity-name test-topic --add-config min.insync.replicas=2
But, I have simply restarted the cluster, and created the topic afresh:
    kafka-topics --create --topic library-events --bootstrap-server broker:9092 --replication-factor 3 --partitions 3 --config min.insync.replicas=2

Modified the spring kafka producer config (in application.yml) properties to have 5 retry attempts, and seek acks from all ISRs
  properties:
    acks: all
    retries: 5

^^ What this does now, is when we try to produce a record, it will seek ack from all the ISR, in our case 2. In normal case, that works fine.

NOW, to reproduce an error scenario:
1. We "down" 2 out of the total 3 brokers that gets spun up during `docker-compose up`.
2. Now we try to produce a record by hitting the `http://localhost:8899/v1/lib/add` endpoint
3. Now, because of the `acks: all` config, the producer tries to get ack from 2 ISRs, as specified in the topic, but since we just have 1 broker, it fails
4. It retries 5 times, and finally errors out, as seen in the log snippet below

    2020-10-17 19:11:02.892  WARN 64031 --- [ad | producer-1] org.apache.kafka.clients.NetworkClient   : [Producer clientId=producer-1] Connection to node -2 (localhost/127.0.0.1:29093) could not be established. Broker may not be available.
    2020-10-17 19:11:02.892  WARN 64031 --- [ad | producer-1] org.apache.kafka.clients.NetworkClient   : [Producer clientId=producer-1] Bootstrap broker localhost:29093 (id: -2 rack: null) disconnected
    2020-10-17 19:11:02.903  WARN 64031 --- [ad | producer-1] org.apache.kafka.clients.NetworkClient   : [Producer clientId=producer-1] Connection to node -3 (localhost/127.0.0.1:29094) could not be established. Broker may not be available.
    2020-10-17 19:11:02.904  WARN 64031 --- [ad | producer-1] org.apache.kafka.clients.NetworkClient   : [Producer clientId=producer-1] Bootstrap broker localhost:29094 (id: -3 rack: null) disconnected
    2020-10-17 19:11:03.007  WARN 64031 --- [ad | producer-1] org.apache.kafka.clients.NetworkClient   : [Producer clientId=producer-1] Connection to node -3 (localhost/127.0.0.1:29094) could not be established. Broker may not be available.
    2020-10-17 19:11:03.007  WARN 64031 --- [ad | producer-1] org.apache.kafka.clients.NetworkClient   : [Producer clientId=producer-1] Bootstrap broker localhost:29094 (id: -3 rack: null) disconnected
    2020-10-17 19:11:03.134  INFO 64031 --- [ad | producer-1] org.apache.kafka.clients.Metadata        : [Producer clientId=producer-1] Cluster ID: jMt3FuTmTc6K3u3DfcEdTw
    2020-10-17 19:11:03.230  WARN 64031 --- [ad | producer-1] o.a.k.clients.producer.internals.Sender  : [Producer clientId=producer-1] Got error produce response with correlation id 3 on topic-partition library-events-0, retrying (4 attempts left). Error: NOT_ENOUGH_REPLICAS
    2020-10-17 19:11:03.338  WARN 64031 --- [ad | producer-1] o.a.k.clients.producer.internals.Sender  : [Producer clientId=producer-1] Got error produce response with correlation id 4 on topic-partition library-events-0, retrying (3 attempts left). Error: NOT_ENOUGH_REPLICAS
    2020-10-17 19:11:03.449  WARN 64031 --- [ad | producer-1] o.a.k.clients.producer.internals.Sender  : [Producer clientId=producer-1] Got error produce response with correlation id 5 on topic-partition library-events-0, retrying (2 attempts left). Error: NOT_ENOUGH_REPLICAS
    2020-10-17 19:11:03.562  WARN 64031 --- [ad | producer-1] o.a.k.clients.producer.internals.Sender  : [Producer clientId=producer-1] Got error produce response with correlation id 6 on topic-partition library-events-0, retrying (1 attempts left). Error: NOT_ENOUGH_REPLICAS
    2020-10-17 19:11:03.672  WARN 64031 --- [ad | producer-1] o.a.k.clients.producer.internals.Sender  : [Producer clientId=producer-1] Got error produce response with correlation id 7 on topic-partition library-events-0, retrying (0 attempts left). Error: NOT_ENOUGH_REPLICAS
    2020-10-17 19:11:03.787 ERROR 64031 --- [ad | producer-1] c.k.b.producer.LibraryEventProducer      : Error Sending message, Exception: Failed to send; nested exception is org.apache.kafka.common.errors.NotEnoughReplicasException: Messages are rejected since there are fewer in-sync replicas than required.
    2020-10-17 19:11:03.788 ERROR 64031 --- [ad | producer-1] c.k.b.producer.LibraryEventProducer      : Error on failure
    2020-10-17 19:11:03.790 ERROR 64031 --- [ad | producer-1] o.s.k.support.LoggingProducerListener    : Exception thrown when sending a message with key='null' and payload='{"libraryEventId":null,"type":"NEW","book":{"bookId":null,"bookName":"Kafka Using Spring Boot","book...' to topic library-events:

    org.apache.kafka.common.errors.NotEnoughReplicasException: Messages are rejected since there are fewer in-sync replicas than required.

^^ this last line is the erroring out message, after 5 retries logged as `[Producer clientId=producer-1] Got error ... retrying (... attempts left)` above

------------------------------------------------------------------------------------------------------------------------
Ways to fix above error scenario
(not implemented in course, but ideas given)
------------------------------------------------------------------------------------------------------------------------

1. Persist failed event records to db, and then try again
2. Publish failed event record to recovery_topic, which will be listened upon by another consumer, which will do the necessary retries

We have the failure handler method in LibraryEventProducer.java:
    private void handleFailure(Integer key, String value, Throwable ex)
^^ we can implement the logic (where to publish in case of error) in here.
